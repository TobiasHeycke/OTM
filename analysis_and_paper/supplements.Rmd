---
title           : |
  Supplementary online material for
    
  *Of Two Minds: A registered replication*

shorttitle      : "Replication of Rydell et al."

author: 
  - name          : Tobias Heycke
    affiliation   : "1,2"
    corresponding : yes    
    address       : "P.O. 122155, 68072 Mannheim, Germany"
    email         : "tobias.heycke@gesis.org"
  - name          : Frederik Aust
    affiliation   : "1"
  - name          : Mahzarin R. Banaji
    affiliation   : 3
  - name          : Jeremy Cone
    affiliation   : 9
  - name          : Pieter Van Dessel
    affiliation   : 5
  - name          : Melissa J. Ferguson
    affiliation   : 8
  - name          : Xiaoqing Hu
    affiliation   : 6    
  - name          : Congjiao Jiang
    affiliation   : 4    
  - name          : Benedek Kurdi
    affiliation   : "3,8"
  - name          : Robert Rydell
    affiliation   : 7
  - name          : Lisa Spitzer
    affiliation   : 1
  - name          : Christoph Stahl
    affiliation   : 1
  - name          : Christine Vitiello
    affiliation   : 4
  - name          : Jan De Houwer
    affiliation   : 5
   

affiliation:
  - id            : 1
    institution   : University of Cologne
  - id            : 2
    institution   : GESIS - Leibniz Institute for the Social Sciences
  - id            : 3
    institution   : Harvard University 
  - id            : 4
    institution   : University of Florida 
  - id            : 5
    institution   : Ghent University 
  - id            : 6
    institution   : The University of Hong Kong 
  - id            : 7
    institution   : Indiana University
  - id            : 8
    institution   : Cornell University
  - id            : 9
    institution   : Williams College
    

bibliography      : ["references.bib", "r-references.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "doc"
output            : papaja::apa6_pdf
---

```{r load-packages, include = FALSE, warning=FALSE}
library("magrittr")
library("tidyr")
library("dplyr")
library("assertthat")

library("papaja")
library("ggplot2")
library("ggforce")
library("cowplot")

library("afex")
library("emmeans")
library("BayesFactor")

# devtools::install_github("arcaldwell49/Superpower")
library("Superpower")

source("https://gist.githubusercontent.com/crsh/be88be19233f1df4542aca900501f0fb/raw/f258053d144681bd4a5b86ed0956da7dc3f380ee/gglegend.R")
source("error_in_variables.R")
source("effect_sizes.R")
```

```{r analysis-preferences}
# Data location
raw_data_path <- "../otm1/results/data_raw/"
processed_data_path <- "../otm1/results/data_processed/"

# Set seed for random number generator
set.seed(315054738)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)

# Use effect coding
options(contrasts = c("contr.sum", "contr.poly"))

# Use multivariate models for emmeans contrasts and post-hoc tests
afex_options(emmeans_model = "multivariate")

# Configure df approximation for mixed model contrasts and post-hoc tests
emm_options(
  lmer.df = "satterthwaite"
  , lmerTest.limit = 22384
)

# Default ggplot theme
theme_set(theme_apa())

# Number of MCMC samples for Bayesian analysis
otm1_n_mcmc_samples <- 1e6
```

```{r cache-preferences}
# Automatically manage cache dependencies
knitr::opts_chunk$set(autodep = TRUE)
knitr::dep_auto()

# Ignore changes to comments in cached chunks
knitr::opts_chunk$set(cache.comments = FALSE)

# Discard cache if random seed changes
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r load-data}
otm1_attitudes <- readRDS(paste0(processed_data_path, "otm1_attitudes_cleaned.rds"))
otm1_iat <- readRDS(paste0(processed_data_path, "otm1_iat_trial_data_cleaned.rds"))
otm1_mem <- readRDS(paste0(processed_data_path, "otm1_memory_cleaned.rds"))
```

\newpage

# Experiment 1

In the following we report details on our power analysis, the model specification used for the Bayesian model comparisons, and additional secondary analyses.
We report results from the linear mixed model analysis of the IAT response times, from prior sensitivity analyses for the Bayesian model comparisons, and from an exploratory analysis of the relationship between the recognition accuracy of briefly flashed words and associative learning.
Table\ \@ref(tab:otm1-participant-table) summarizes the participants' demographics separately for each location of data collection.

(ref:otm1-participant-table) Participant demographics by location.

(ref:otm1-participant-table-note) Mean age is given with range in brackets.

```{r otm1-participant-table, results = "asis"}
otm1_mem %>% 
  group_by(Location) %>% 
  summarize(
    age_mean = mean(Age, na.rm = T)
    , age_min = min(Age)
    , age_max = max(Age)
    , "Female (\\%)" = printnum(sum(grepl("female", Sex)) / length(Accuracy) * 100)
    , "$n$" = length(Accuracy)
  ) %>%
  mutate(Age = paste0(printnum(age_mean), " [", age_min, ", ", age_max, "]")) %>%
  select(Location, Age, "Female (\\%)", "$n$") %>%
  apa_table(
    caption = "(ref:otm1-participant-table)"
    , note = "(ref:otm1-participant-table-note)"
    , align = "lccc"
    , escape = FALSE
    , placement = "h"
  )
```


## Power analysis

The prediction, which is supported by all previous empirical reports, is a crossed disordinal interaction between the factor *learning block* and the control factor *valence order*.
Therefore, our power analyses focuses on this interaction and the theoretically relevant simple effects of *learning block*.
We estimate the sensitivity of our design using the R-package *Superpower* [@R-Superpower] for the contrast analyes using the R-package *emmeans* [@R-emmeans].

```{r rydell-effects, cache = TRUE}
rydell_pes <- 0.1
rydell_f <- 4.78
rydell_n <- 50
rydell_dz <- pes_to_dz(rydell_pes, rydell_n, n_groups = 2)

heycke_t <- c(2.54, 1.45)
heycke_n <- c(28, 26)
heycke_dz <- heycke_t / sqrt(heycke_n)
heycke_pes <- dz_to_pes(heycke_dz, heycke_n, n_groups = 1)
```

```{r otm1-power, cache = TRUE}
otm1_power <- function(f, rm_cor, n = 76, alpha = 0.05) {
  contrast_code <- tibble::tibble(
    a = rep(contr.sum(2), 2)
    , b = rep(contr.sum(2), each = 2)
    , x = a*b * -1
  )
  
  mu <- with(
    contrast_code
    , f * x +
      - 0.25 * a
  )

  cor_mat <- diag(4)
  triangular <- c(0, 1, 0, 0, 1, 0)
  cor_mat[upper.tri(cor_mat)] <- triangular * rm_cor
  cor_mat[lower.tri(cor_mat)] <- triangular * rm_cor
  
  string <- "2w*2b"
  labelnames <- c("block", "1", "2", "order", "n-p", "p-n")
  
  design_result <- ANOVA_design(
    design = string
    , n = n
    , mu = mu
    , sd = 1
    , r = cor_mat
    , labelnames = labelnames
  )
  
  otm1_power <- ANOVA_exact(
    design_result
    , alpha_level = alpha
    , emm = TRUE
    , verbose = FALSE
    , contrast_type = "pairwise"
    , emm_comp = "block | order"
  )
  
  otm1_power
}

otm1_n <- 76

# 95% power
otm1_pes <- 0.08068
otm1_dz <- pes_to_dz(otm1_pes, otm1_n, n_groups = 2)
# rho <- 0.5
# f <- dz_to_f(otm1_dz, rho)
# 
# otm1_contrast_power <- otm1_power(f = f, n = otm1_n, rm_cor = rho)$emmeans$contrasts %>%
#   emmeans_power(alpha = 0.05) %>%
#   .[1, ]
# otm1_contrast_power

# 80% power
otm1_pes2 <- 0.050336
otm1_dz2 <- pes_to_dz(otm1_pes2, otm1_n, n_groups = 2)
# f <- dz_to_f(otm1_dz2, rho)
# 
# otm1_contrast_power2 <- otm1_power(f = f, n = otm1_n, rm_cor = rho)$emmeans$contrasts %>%
#   emmeans_power(alpha = 0.05) %>%
#   .[1, ]
# otm1_contrast_power2
```


@rydell_two_2006 found the smallest learning block difference for IAT scores, $\hat\eta_p^2 = `r printp(rydell_pes)`$, $d_z \approx `r rydell_dz`$.
The learning block differences reported by @heycke_two_2018 were of similar magnitude but with an opposite sign.
Across all labs ($N = 152$), our design has 95% power to detect effects as small as $\eta^2_p = `r printp(otm1_pes)`$ ($\delta_z = `r otm1_dz`$) and 80% power to detect effects as small as $\eta^2_p = `r printp(otm1_pes2)`$ ($\delta_z = `r otm1_dz2`$) in two-sided tests.
Thus, our design is sufficiently sensitive to detect (or rule out) differences `r round((1 - otm1_dz/rydell_dz) * 100)`% smaller than the smallest learning block difference reported by @rydell_two_2006.
Figure\ \@ref(fig:otm1-power-curve) illustrates the implied sensitivity in units of Cohen's $\delta$ depending on the assumed repeated-measures correlation $\rho$.


(ref:otm1-power-curve) Sensitivity curves for learning block contrasts in Experiment 1 depending on the assumed repeated measures correlation.
The grey dashed line at the top represents the smallest estimate of the learning block difference reported by Rydell et al. [$d_z = `r rydell_dz`$; -@rydell_two_2006].

```{r otm1-power-curve, fig.width = 5.75, fig.height = 4, fig.cap = "(ref:otm1-power-curve)"}
rydell_power_curve <- tibble(
  rho = seq(-1, 1, length.out = 100)
  , delta = c(
    dz_to_f(rydell_dz, unique(rho)) * 2
  )
  , power = NA
)

otm1_power_curve <- tibble(
  rho = rep(seq(-1, 1, length.out = 100), 2)
  , delta = c(
    dz_to_f(otm1_dz, unique(rho)) * 2
    , dz_to_f(otm1_dz2, unique(rho)) * 2
  )
  , power = rep(c("0.95", "0.80"), each = length(rho)/2)
)

otm1_power_curve %>% 
  mutate(
    power = relevel(factor(power), "0.95")
  ) %>% 
  ggplot(aes(x = rho, y = delta, group = power, linetype = power)) +
  geom_line(data = rydell_power_curve, aes(group = power), color = grey(0.8), linetype = "dashed") +
  geom_line() +
  ylim(c(0, 1)) +
  scale_linetype_discrete(
    labels = c(
      bquote(.(unique(otm1_power_curve$power)[1])~~(delta[z] == .(printnum(otm1_dz))))
      , bquote(.(unique(otm1_power_curve$power)[2])~~(delta[z] == .(printnum(otm1_dz2))))
    )
  ) +
  labs(
    x = bquote("Correlation between learning blocks ["*rho*"]")
    , y = bquote("Learning block difference [Cohen's"~delta*"]")
    , color = "Contrast"
    , linetype = bquote("Power"~"["*1 - beta*"]")
  ) +
  ggtitle("", subtitle = bquote(list(alpha ==~".05", italic(N) == .(otm1_n*2)))) +
  theme_apa(box = TRUE) + 
  theme(
    panel.grid.major.x = element_line()
    , panel.grid.major.y = element_line()
    , panel.grid.minor.x = element_line()
    , panel.grid.minor.y = element_line()
    , legend.justification = "top"
  )
```


```{r otm1-power-curves, eval = FALSE}
otm1_power_curve <- expand.grid(
  n = 76
  , rho = seq(-0.9, 0.9, length.out = 10)
  , alpha = 0.05
  , d = seq(0, 0.9, length.out = 96)
) %>%
  as_tibble %>% 
  mutate(
    f = d/2
    , generalized_eta_squared = f^2 / (1 + f^2)
    , interaction = NULL
    , contrast = NULL
  )

otm1_power_curve[, c("interaction", "contrast")] <- t(apply(
  otm1_power_curve
  , 1
  , function(x) {
    power <- otm1_power(n = x["n"], f = x["f"], rm_cor = x["rho"], alpha = x["alpha"])
    
    c(
      power_int = power$main_result["order:block", "power"] / 100
      , power_contrast = emmeans_power(power$emmeans$contrasts, alpha_level = x["alpha"])[1, "power"] / 100
    )
  }
))
```

```{r otm1-power-curves-plot, fig.width = 7, fig.height = 4.5, eval = FALSE}
otm1_annotations <- data.frame(
  label = c(
    paste0("list(italic(N) ==", unique(otm1_power_curve$n)*2, ", alpha == 0.05, epsilon == 1)")
    , paste0("list(italic(n) ==", unique(otm1_power_curve$n), ", alpha == 0.05)")
  )
  , test = factor(
      c("interaction", "contrast")
      , levels = c("interaction", "contrast")
      , labels = c("italic('Learning block')%*%italic('Valence order')", "atop(italic('Learning block')*'-contrasts', 'by'~italic('Valence order'))")
    )
  )

pivot_longer(
  otm1_power_curve
  , cols = c("interaction", "contrast")
  , names_to = "test"
  , values_to = "power"
) %>% 
  mutate(
    test = factor(
      test
      , levels = c("interaction", "contrast")
      , labels = c("italic('Learning block')%*%italic('Valence order')", "atop(italic('Learning block')*'-contrasts', 'by'~italic('Valence order'))")
    )
  ) %>% 
  filter(power <= 0.99) %>% 
  ggplot(aes(x = d, y = power, color = rho, group = rho)) +
    geom_hline(yintercept = c(0.8, 0.95), linetype = "solid", color = grey(0.75), size = 0.25) +
    geom_line(linetype = "solid") +
    geom_text(data = otm1_annotations, aes(x = 0.87, y = 0.09, label = label), inherit.aes = FALSE, parse = TRUE, hjust = 0, vjust = 1) +
    scale_x_continuous(
      name = bquote("Cohen's"~delta)
      , position = "top"
      , breaks = function(x) seq(0, x[2], 0.1)
      , sec.axis = sec_axis(trans = ~ .^2 / (4 + .^2), name = bquote(eta[G]^2), breaks = function(x) { b <- seq(0, (2*sqrt(x[2]))/sqrt(1 - x[2]), 0.1); b^2 / (4 + b^2) }, labels = printp)
    ) +
    scale_y_continuous(
      name = bquote(1 - beta)
      , breaks = function(x) seq(0.05, 0.95, 0.15)
      , labels = function(...) printp(..., digits = 2)
    ) +
    scale_color_viridis_c(
      name = bquote(rho)
      , end = 0.9
      , guide = guide_colorbar(title.position = "top", title.hjust = 0.5, direction = "vertical", barheight = unit(75, "pt"), barwidth = unit(12, "pt"), reverse = TRUE)
      , limits = c(-1, 1)
    ) +
    facet_grid(~ test, labeller = label_parsed) +
    guides(linetype = FALSE) +
    coord_flip(ylim = c(0.05, 1)) +
    papaja::theme_apa(box = TRUE) +
    theme(
      legend.position = c(0, 0.88)
      , legend.justification = c(0, 1)
      , legend.background = element_blank()
      , plot.title = element_text(margin = margin(b = rel(5)))
      , strip.text.x = element_text(margin = margin(b = rel(10)))
      , panel.grid.major.y = element_line(linetype = "22")
    )
```


## Mixed model analysis

The ANOVA of IAT scores reported in the main text ignores potential systematic trial-to-trial variability in IAT response latencies due to stimuli.
Any such systematic but unaccounted-for variance can inflate test statistics and yield underestimated $p$ values as well as underestimated confidence intervals.
We, therefore, also conducted a linear mixed model analysis of response times with crossed random effects for participants and items to ensure that our conclusion are not contingent on inadvertent stimulus effects [for details see @wolsiefer_modeling_2017].
For this analysis we excluded participants with error rates across all blocks larger than 50% or who responded faster than 300 ms on at least 10% of all trials.
We additionally discarded trials in which responses were faster than 400 ms or slower than 10 s.
These exclusion criteria are the same as those used by @wolsiefer_modeling_2017.

```{r otm1-lmer-exclusion, cache = TRUE}
lme_error_exclusion <- otm1_iat %>%
  group_by(ParticipantNumber) %>%
  summarize(Correct = mean(Correct), fast_rate = mean(RT < 0.3)) %>%
  filter(Correct < 0.5 & fast_rate > 0.1) %$% ParticipantNumber

if(length(lme_error_exclusion) > 0) {
  otm1_iat <- otm1_iat %>%
    filter(!ParticipantNumber %in% otm1_lme_exclusion)
}

otm1_iat <- otm1_iat %>%
  # Wolsiefer et al. (2017, p. 1196; doi: 10.3758/s13428-016-0779-0)
  filter(RT > 0.4 & RT < 10) %>%
  # Wolsiefer et al. (2017, p. 1198; doi: 10.3758/s13428-016-0779-0)
  group_by(ParticipantNumber, Block) %>%
  mutate(RTD = RT / sd(RT)) %>%
  ungroup
```

We analyzed standardized response latencies, that is, the time that elapsed between stimulus presentation and *correct* response divided by the standard deviation of all response latencies in a given block, Figure\ \@ref(fig:otm1-iat-plot).
To assess the reversal of the response mapping effect, we contrasted the common response mapping of Bob and negative words with the common mapping of Bob and positive words.
Hence, larger values represent more favorable indirect evaluations.

(ref:otm1-iat-plot) Standardized IAT response latencies across learning blocks.
Black-rimmed points represent condition means, error bars represent 95% bootstrap confidence intervals based on 10,000 samples.

```{r otm1-iat-plot, fig.cap = "(ref:otm1-iat-plot)"}
otm1_results_legend <- guide_legend(
    title = expression(atop("Valence order", atop(scriptstyle("Behavioral information"), scriptstyle("(Briefly flashed words)"))))
    , title.position = "top"
    , title.hjust = 0.5
    , reverse = TRUE
    , keyheight = 2
  )

my_labeller <- function(x, ...) label_both(x, sep = " ", ...)

otm1_iat %>%
  mutate(ValenceBlock = ifelse(ValenceBlock == "Positive-negative", " Positive-negative\n(Negative-positive)", " Negative-positive\n(Positive-negative)")) %>%
  ggplot(aes(x = Congruent, y = RTD, group = ValenceBlock, color = ValenceBlock, shape = ValenceBlock, fill = ValenceBlock)) +
  stat_summary(fun.y = mean, geom = "line", position = position_dodge(0.1)) +
  stat_summary(fun.data = mean_cl_boot, position = position_dodge(0.1), color = "black", fun.args = list(B = 10000)) +
  scale_color_brewer(palette = "Set1", guide = otm1_results_legend) +
  scale_fill_brewer(palette = "Set1", guide = otm1_results_legend) +
  scale_shape_manual(values = c(21, 23), guide = otm1_results_legend) +
  labs(x = "Response mapping", y = "Standardized response time") +
  facet_grid(~ Block, labeller = my_labeller) +
  theme(
    legend.position = c(0.4, 0.75)
    , strip.background = element_blank()
  )
```

```{r otm1-lmer, cache = TRUE, warning = FALSE}
# Wolsiefer et al. (2017, Erratum; doi: 10.3758/s13428-017-0897-3)
otm1_iat_lmer_formula <- RTD ~ Congruent * Block * ValenceBlock *
  (Category + wordType + imageType) +
  (Congruent * Block | ParticipantNumber) +
  (Congruent * Block * ValenceBlock | translatedStimulus)

otm1_iat_lmerTest <- lmerTest::lmer(
  otm1_iat_lmer_formula
  , data = otm1_iat
  , control = lmerControl(
    optCtrl = list(maxfun = 10 * 143^2)
  )
)

otm1_iat_lmerTest_summary <- summary(otm1_iat_lmerTest)
```

(ref:otm1-lmer-fixed-effects-caption) Fixed effect estimates of the linear mixed model analysis of standardized IAT response times.

(ref:otm1-lmer-fixed-effects-note) The model additionally included random participant and item effects with random intercepts and random slopes for all manipulations during the learning procedure and their interactions.

```{r otm1-lmer-table-fixed-effects, results = "asis"}
otm1_iat_lmerTest_summary$coefficients %>%
  as.data.frame %>%
  rename(
    "$b$" = "Estimate"
    , "SE" = "Std. Error"
    , "$df$" = "df"
    , "$t$" = "t value"
    , "$p$" = "Pr(>|t|)"
  ) %>%
  mutate(
    Effect = papaja:::prettify_terms(rownames(.))
    , Effect = gsub("1", "", Effect)
    , Effect = gsub("\\bBlock", "Learning block", Effect)
    , Effect = gsub("ValenceBlock", "Valence order", Effect)
    , Effect = gsub("Block", " block", Effect)
    , Effect = gsub("Type", " type", Effect)
    , Effect = gsub("Congruent", " Response mapping", Effect)
  ) %>%
  printnum(
    digits = c(2, 2, 2, 2, 3, 0)
    , gt1 = c(TRUE, TRUE, TRUE, TRUE, FALSE, FALSE)
    , zero = c(TRUE, TRUE, TRUE, TRUE, FALSE, FALSE)
  ) %>%
  select(Effect, `$b$`, `SE`, `$t$`, `$df$`, `$p$`) %>%
  apa_table(
    caption = "(ref:otm1-lmer-fixed-effects-caption)"
    , note = "(ref:otm1-lmer-fixed-effects-note)"
    , align = "lrrrrr"
    , midrules = c(7, 19, 29)
    , escape = FALSE
    , landscape = TRUE
  )
```

```{r explore-salience-asymmetry, eval = FALSE}
# Main effect of response mapping is significant for both image types
otm1_iat_lmerTest %>%
  emmeans(specs = ~ Congruent | imageType, at = list(imageType = c(-1, 1))) %>%
  pairs()
```


(ref:otm1-lmer-random-effects-caption) Random effect estimates and correlations of the linear mixed model analysis of standardized IAT response times.

(ref:otm1-lmer-random-effects-note) We report the estimated standard deviations in the main diagonals and the correlations in the off-diagnoals. The percentages of variance for the random effects were calculated by dividing each variance component by the total random variance, i.e., the sum of the random-effect variances.

```{r otm1-lmer-table-random-effects, results = "asis"}
fix_term_names <- function(x) {
  fixed_table <- attr(x, "correlation")
  
  term_names <- x %>%
    colnames %>%
    papaja:::prettify_terms(.) %>%
    gsub("1", "", .) %>% 
    gsub("\\bBlock", "Learning block", .) %>% 
    gsub("ValenceBlock", "Valence order", .) %>% 
    gsub("Block", " block", .) %>% 
    gsub("Type", " type", .) %>% 
    gsub("Congruent", " Response mapping", .)
  

  diag(fixed_table) <- attr(x, "stddev")
  
  fixed_table <- as.data.frame(fixed_table) %>%
    printnum
  
  fixed_table[lower.tri(fixed_table)] <- ""
  
  dimnames(fixed_table) <- list(
    paste0(1:length(term_names), ". ", term_names)
    , paste0(1:length(term_names), ". ")
  )
  
  fixed_table
}

random_effect_tables <- lapply(otm1_iat_lmerTest_summary$varcor, fix_term_names)
names(random_effect_tables) <- c("Participant", "Stimulus")

stimulus_effect_names <- colnames(random_effect_tables$Stimulus)

random_variances <- lapply(otm1_iat_lmerTest_summary$varcor, diag) %>%
  unlist %>%
  unname

variance_explained <- (random_variances / sum(random_variances)) %>%
  printnum(gt1 = FALSE)

random_effect_tables$Participant <- cbind(
  "\\% of variance" = variance_explained[1:nrow(random_effect_tables$Participant)]
  , random_effect_tables$Participant
)

random_effect_tables$Stimulus <- cbind(
  "\\% of variance" = variance_explained[1:nrow(random_effect_tables$Stimulus) + nrow(random_effect_tables$Participant)]
  , random_effect_tables$Stimulus
)

random_effect_tables$Participant[, stimulus_effect_names[!stimulus_effect_names %in% colnames(random_effect_tables$Participant)]] <- ""

apa_table(
  random_effect_tables
  , caption = "(ref:otm1-lmer-random-effects-caption)"
  , note = "(ref:otm1-lmer-random-effects-note)"
  , align = "lcrrrrr"
  , midrules = 5
  , escape = FALSE
  , landscape = TRUE
  , font_size = "small"
)
```

In line with the ANOVA results, we found the expected three-way interaction between *Response mapping*, *Valence order*, and *Learning block*; the interaction was moderated by the type of stimulus that participants responded to (pictures of Bob and non-Bobs vs. positive and negative words; *Category*), Table\ \@ref(tab:otm1-lmer-table-fixed-effects) and \@ref(tab:otm1-lmer-table-random-effects).
The three-way interaction prompted us to test the differences between response mapping effects in the first and second learning block for each valence order.

```{r otm1-lmer-emm, cache = TRUE, message = FALSE, warning = FALSE}
otm1_iat_lmer <- lme4::lmer(
  otm1_iat_lmer_formula
  , data = otm1_iat
  , control = lmerControl(
    optCtrl = list(maxfun = 10 * 143^2)
  )
)

otm1_iat_lmer_emm <- emmeans(
  otm1_iat_lmer
  , ~ Congruent * Block * ValenceBlock
)
```

```{r otm1-lmer-emm-2, cache = TRUE, results = "asis"}
otm1_iat_lmer_contrasts <- contrast(
  otm1_iat_lmer_emm
  , list(
    "Negative-positive" = c(1, -1, -1, 1, 0, 0, 0, 0)
    , "Positive-negative" = c(0, 0, 0, 0, 1, -1, -1, 1)
  )
) %>%
  apa_print
```

In line with the conventional ANOVA analysis, we found that response time differences suggested more favorable evaluations of Bob after the first than after the second block when the behavioral information was first positive and later negative, `r otm1_iat_lmer_contrasts$full_result$Positive_negative`.
Vice versa, response time differences suggested more favorable evaluations after the second than after the first block when descriptions of Bob were first negative and later positive, `r otm1_iat_lmer_contrasts$full_result$Negative_positive`.
Again, these results indicate that the self-reported evaluations and IAT scores were consistent.

Due to the significant four-way interaction, we additionally explored these contrasts separately for responses to pictures of Bob vs. non-Bobs and positive vs. negative words, Table \ \@ref(tab:otm1-lmer-emm-3-table).
We found consistent changes in response mapping effects for both pictures and words, albeit the effects were larger for words. 

```{r otm1-lmer-emm-3, cache = TRUE, warning = FALSE}
otm1_iat_lmer_emm2 <- emmeans(
  otm1_iat_lmer
  , ~ Congruent * Block * ValenceBlock | Category
)

otm1_iat_lmer_contrasts2 <- contrast(
  otm1_iat_lmer_emm2
  , list(
    "Negative-positive" = c(1, -1, -1, 1, 0, 0, 0, 0)
    , "Positive-negative" = c(0, 0, 0, 0, 1, -1, -1, 1)
  )
  , adjust = "Tukey"
) %>%
  apa_print

variable_label(otm1_iat_lmer_contrasts2$table$contrast) <- "Valence order"
```

(ref:otm1-iat-lmer-contrasts2) Post-hoc tests of changes in response mapping effects across blocks separately for pictures and words for standardized IAT response times.

(ref:otm1-iat-lmer-contrasts2-note) $p$ values were Tukey-corrected for two comparisons.

```{r otm1-lmer-emm-3-table}
otm1_iat_lmer_contrasts2 %$%
  table %>%
  select(-matches("Category|df|statistic|p.value"), statistic, df, p.value) %>%
  apa_table(
    caption = "(ref:otm1-iat-lmer-contrasts2)"
    , note = "(ref:otm1-iat-lmer-contrasts2-note)"
    , align = "lrcrrr"
    , stub_indents = list("Pictures" = 1:2, "Words" = 3:4)
    , midrule = 3
    , escape = FALSE
    , row.names = FALSE
  )
```



## Bayesian model comparison

We implemented the unconstrained model as a hierarchical linear model that encompasses each of the other models as special cases:

$$
\begin{aligned}
\hat y_{ijk} = & \mu + \nu_i + \eta_l x_{1il} + \\
          & (\alpha + \tau_l x_{1il}) x_{2j} x_{3k} + \\
          & (\beta + \upsilon_l x_{1il}) (1 -  x_{2j}) x_{3k}
\end{aligned}
$$

The model predicts the $i$th participant's response to evaluation measure $j$ in the experimental block $k$.
Responses are predicted as a combination of a grand mean $\mu$, random participant intercepts $\nu_i$ (i.e., habitually higher or lower evaluations), a main effect of the labs $\eta_l$, and simple effects of learning block for rating scores ($\alpha$) and IAT score ($\beta$).
Additionally, we allowed the simple effects to be moderated by the labs ($\tau_l$ and $\upsilon_l$ represent the lab-specific deviations from the overall simple effects).
The model does not include a main effect of evaluative measure because any mean differences between evaluative measures were leveled by the by-measure $z$ standardization.
$x_{1il}$ represents $l$ effect coded variables that indicate which lab participant $i$ belongs to; $x_{2j}$ indicates the evaluative measure (1 for rating score and 0 for IAT score), such that $\alpha + \tau_l$ is only relevant for rating scores and $\beta + \upsilon_l$ is only relevant for IAT scores; $x_{3k}$ is an effect coded variable that is set to 0.5 for block 1 and -0.5 for block 2.

This model allowed us to place priors on the simple effects (in units of standardized mean differences $d$) for each evaluative measure and implement the theoretically motivated order constraints:

$$
\begin{aligned}
\mathcal{M}_\textrm{No effect}:~ & \delta_\alpha = 0 \\ & \delta_\beta = 0 \\
\mathcal{M}_\textrm{One mind}:~ & \delta_\alpha \sim \textrm{Positive-Half-Cauchy}(r = \sqrt2/2) \\ & \delta_\beta \sim \textrm{Positive-Half-Cauchy}(r = \sqrt2/2) \\
\mathcal{M}_\textrm{Two minds}:~ & \delta_\alpha \sim \textrm{Positive-Half-Cauchy}(r = \sqrt2/2) \\ & \delta_\beta \sim \textrm{Negative-Half-Cauchy}(r = \sqrt2/2) \\
\mathcal{M}_\textrm{Any effect}:~ & \delta_\alpha \sim \textrm{Cauchy}(r = \sqrt2/2) \\ & \delta_\beta \sim \textrm{Cauchy}(r = \sqrt2/2)
\end{aligned}
$$

Additionally, we placed default multivariate Cauchy priors ($r = \sqrt2/2$) on lab main effects $\eta_l$ as well as on lab effects on evaluative differences between blocks for rating scores ($\tau_l$) and IAT scores ($\upsilon_l$).

To formally assess whether the data from all labs exhibited consistent effects we added another model that enforced the order constraint of $\mathcal{M}_\textrm{One mind}$ and $\mathcal{M}_\textrm{Two minds}$ not only for the average block effects ($\alpha$ and $\beta$) but for each lab individually (i.e., $\alpha_l = \alpha + \tau_l$ and $\beta_l = \beta + \upsilon_l$; $\mathcal{M}_\textrm{One mind everywhere}$ and $\mathcal{M}_\textrm{Two minds everywhere}$).

For the analyses we drew 1 million samples to estimate the postrior distribtution of model parameters.
Because the draws from the posterior distribution are used to estimate the Bayes factors for model comparisons that involve order constraints [@klugkist_inequality_2005], the number of draws implies upper and lower bounds on some of the reported Bayes factors.
Most notably, as a direct consequence of the number MCMC samples the $\textrm{BF}_{\mathcal{M}_\textrm{One mind}/\mathcal{M}_\textrm{Two minds}} \in [\frac{1}{1 \times 10^6}, 1 \times 10^6]$.

### Prior sensitivity analysis

Bayesian model comparison by Bayes factors are by definition sensitive to the specified prior distributions.
To ensure that our inference is not contigent on our choice of piors we conducted prior sensitivity analyses for our key results.

#### Direct and indirect evaluations

Our choice of piors for the simple effects of learning block for rating scores ($\alpha$) and IAT score ($\beta$) could be viewed as either overly optimistic or pessimistic.
The prior on simple rating score effects places considerable probability mass on effects $d$ < 0.707 although the previously reported effects were very large.
Similarly, placing the same prior on the simple effects for rating and IAT scores could be criticized because the previously reported IAT score effects were considerably smaller than those of rating scores.

```{r otm1-prior-sensitivity-replication, cache = TRUE, warning = FALSE}
manuscript_cache <- "manuscript_cache/latex/"

otm1_attitudes_collapsed <- knitr::load_cache(label = "otm1-bayesian-replication-restructure", object = "otm1_attitudes_collapsed", path = manuscript_cache)
otm1_n <- as.integer(nlevels(otm1_attitudes_collapsed$ParticipantNumber))

otm1_null_model_matrix <- knitr::load_cache(label = "otm1-bayesian-replication-design-matrix", object = "otm1_null_model_matrix", path = manuscript_cache)
otm1_unconstrained_model_matrix <- knitr::load_cache(label = "otm1-bayesian-replication-design-matrix", object = "otm1_unconstrained_model_matrix", path = manuscript_cache)

replication_rscales <- expand.grid(
  iat = seq(0.5, 1, length.out = 3)
  , rating = seq(sqrt(2)/2, 2, length.out = 3)
) %>% 
  filter(iat <= rating) %>%
  as.matrix %>% 
  `/`(., sqrt(2)) %>% # Rescale to ANOVA specification (see https://forum.cogsci.nl/discussion/3746/bayesfactor-scale-of-cauchy-prior-in-t-tests-and-anova)
  as.data.frame

replication_rscales$bf_one_mind_two_minds <- NA
replication_rscales$bf_one_mind_any_effect <- NA

for(i in 1:nrow(replication_rscales)) {
  i_rscales <- c(
    eta = 0.5
    , alpha = unname(replication_rscales[i, "rating"])
    , tau = 0.5
    , beta = unname(replication_rscales[i, "iat"])
    , upsilon = 0.5
  )
  
  
  # No effect model
  otm1_no_effect <- nWayAOV(
    y = otm1_attitudes_collapsed$Attitude
    , X = otm1_null_model_matrix
    , gMap = c(
      eta = rep(0, 2)
      , nu = rep(1, otm1_n)
    )
    , rscale = c(fixed = 0.5, random = 1)
    , iterations = otm1_n_mcmc_samples
  )
  
  # Fixed effect model
  otm1_unconstrained <- nWayAOV(
    y = otm1_attitudes_collapsed$Attitude
    , X = otm1_unconstrained_model_matrix
    , gMap = c(
      eta = rep(0, 2)
      , alpha = 1, tau = rep(2, 2)
      , beta = 3, upsilon = rep(4, 2)
      , nu = rep(5, otm1_n)
    )
    , rscale = c(fixed = i_rscales, random = 1)
    , iterations = otm1_n_mcmc_samples
  )
  otm1_unconstrained_bf <- exp(otm1_unconstrained$bf - otm1_no_effect$bf)
  
  
  # Fixed effect model
  otm1_unconstrained_samples <- nWayAOV(
    y = otm1_attitudes_collapsed$Attitude
    , X = otm1_unconstrained_model_matrix
    , gMap = c(
      eta = rep(0, 2)
      , alpha = 1, tau = rep(2, 2)
      , beta = 3, upsilon = rep(4, 2)
      , nu = rep(5, otm1_n)
    )
    , rscale = c(fixed = i_rscales, random = 1)
    , posterior = TRUE
    , iterations = otm1_n_mcmc_samples
  )
  colnames(otm1_unconstrained_samples)[1:ncol(otm1_fixed_effects_matrix) + 1] <- colnames(otm1_fixed_effects_matrix)
  
  
  ## Same direction
  otm1_same_direction_boost <- 4 * (
    (sum(
      otm1_unconstrained_samples[, "alpha"] > 0 &
      otm1_unconstrained_samples[, "beta"] > 0
    ) + 1) / (nrow(otm1_unconstrained_samples) + 2)
  )
  otm1_same_direction_bf <- otm1_unconstrained_bf * otm1_same_direction_boost
  
  ## Opposite direction
  otm1_opposite_direction_boost <- 4 * (
    (sum(
      otm1_unconstrained_samples[, "alpha"] > 0 &
      otm1_unconstrained_samples[, "beta"] < 0
    ) + 1) / (nrow(otm1_unconstrained_samples) + 2)
  )
  otm1_opposite_direction_bf <- otm1_unconstrained_bf * otm1_opposite_direction_boost
  
  rm("otm1_unconstrained_samples") # Go easy on GitHub
  
  replication_rscales$bf_one_mind_two_minds[i] <- otm1_same_direction_bf / otm1_opposite_direction_bf
  replication_rscales$bf_one_mind_any_effect[i] <- otm1_same_direction_bf / otm1_unconstrained_bf
}
```

We, therefore, varied the scale for the Cauchy priors on the simple effects in the ranges of $`r min(replication_rscales$rating)` < r_\alpha < `r max(replication_rscales$rating)`$ and $`r min(replication_rscales$iat)` < r_\beta < `r max(replication_rscales$iat)`$ for rating and IAT scores, respectively.
Considering results previous studies, we limited our reanalysis to combinations where the prior scale was larger for rating than for IAT effects.
The results of the prior sensitvity analysis reassure us that our inference is robust to a wide range and combination of scales of the default Cauchy priors, see Table\ \@ref(tab:otm1-prior-sensitivity-replication-table).
The Bayes factors were not affected by the scale of the priors to any meaningful degree.
This is because our data are informative enough to overwhelm the priors and because these Bayes factors primarily depend on the shape and location of the posterior distribution, not the prior distributions [@klugkist_bayesian_2005].


(ref:otm1-prior-sensitivity-replication-table) Results of the prior sensitivity analysis for the Bayesian model comparisons of primary interest.

(ref:otm1-prior-sensitivity-replication-table-note) The Bayes factor (BF) in favor of $\mathcal{M}_\textrm{One mind}$ relative to $\mathcal{M}_\textrm{Any effect}$ is bounded within the range of $[0, 4]$ (see footnote 1 in the main article).
$r_\alpha$ and $r_\beta$ denote the scale for the Cauchy prior on the simple effects of learning block for rating scores ($\alpha$) and IAT scores ($\beta$), respectively (in units of standard deviations).

```{r otm1-prior-sensitivity-replication-table}
replication_rscales <- replication_rscales %>%
  printnum(format = c("f", "f", "e", "f")) %>%
  mutate(
    bf_one_mind_two_minds = papaja:::typeset_scientific(bf_one_mind_two_minds)
    , bf_one_mind_two_minds = paste0("$", bf_one_mind_two_minds, "$")
  ) %>%
  select(rating, everything())

variable_labels(replication_rscales) <- c(
  "rating" = "$r_\\alpha$"
  , "iat" = "$r_\\beta$"
  , "bf_one_mind_two_minds" = "$\\textrm{BF}_{\\mathcal{M}_\\textrm{One mind}/\\mathcal{M}_\\textrm{Two minds}}$"
  , "bf_one_mind_any_effect" = "$\\textrm{BF}_{\\mathcal{M}_\\textrm{One mind}/\\mathcal{M}_\\textrm{Any effect}}$"
)

apa_table(
  replication_rscales
  , caption = "(ref:otm1-prior-sensitivity-replication-table)"
  , note = "(ref:otm1-prior-sensitivity-replication-table-note)"
  , align = "c"
  , escape = FALSE
)
```



#### Recognition task

```{r otm1-prior-sensitivity-recognition}
rec_rscales <- seq(from = 0.5, to = 1, length.out = 2)

rec_bfs <- sapply(
  , X = rec_rscales
  , FUN = function(i, ...) { as.vector(ttestBF(otm1_mem$Accuracy - 0.5, mu = 0, rscale = i, nullInterval = c(0, Inf))[1]) }
)

rec_bfs_res <- rec_bfs %>%
  printnum(digits = 2, format = "e") %>%
  papaja:::typeset_scientific()
```

To test the robustness of our inference regarding participants recognition accuracy we varied the scale $r$ of the Cauchy prior in a wide interval of $[`r min(rec_rscales)`, `r max(rec_rscales)`]$.
The resulting Bayes factors were $`r rec_bfs_res[2]`< \textrm{BF}_{10} < `r rec_bfs_res[1]`$ and thus varied by a factor of `r rec_bfs[1] / rec_bfs[2]`.
These results again reassure that our inference is robust to a wide range of scales of the default Cauchy prior.


## Word recogniton and indirect evaluations

(ref:otm1-rydell-recognition-plot) Black-rimmed points represent condition means, error bars represent 95% bootstrap confidence intervals based on 10,000 samples.
Small points represent individual participants' accuracy.
Violins represent kernel density estimates of sample distributions.

```{r otm1-rydell-recognition-plot, fig.cap = "(ref:otm1-rydell-recognition-plot)", fig.width = 3.5, fig.height = 2.5}
otm1_mem %>%
  ggplot(aes(x = Location, y = Accuracy, shape = Location, fill = Location)) +
  geom_hline(yintercept = 0.5, color = grey(0.5)) +
  geom_violin(alpha = 0.3) +
  geom_sina(alpha = 0.5, size = 1) +
  stat_summary(fun.data = mean_cl_boot, color = "black", fun.args = list(B = 10000)) +
  scale_shape_manual(values = c(21, 23, 24)) +
  scale_color_manual(values = c(RColorBrewer::brewer.pal(3, "Set1"), "#000000")) +
  scale_fill_manual(values = c(RColorBrewer::brewer.pal(3, "Set1"), "#000000")) +
  labs(
    x = "Location"
    , y = "Recognition accuracy"
  ) +
  theme(legend.position = "none")
```

In contrast to the original results reported by @rydell_two_2006, the recognition accuracy of briefly flashed words was above chance  in this study, Figure\ \@ref(fig:otm1-rydell-recognition-plot).
Memory for these words may, thus, have interfered with the associative learning process and prevented the predicted reversal of the IAT score differences.
We, therefore, performed an exploratory regression analysis of the recognition accuracy of briefly flashed words and the IAT score difference between blocks used in the Bayesian analysis above.
Positive IAT score differences represent a more favorable evaluation after the block in which Bob was paired with positive behavioral information and briefly flashed negative words.
Conversely, negative IAT score differences between blocks indicate that the IAT effects reflect the valence of the briefly flashed words.
If word recognition indeed obstructed the associative learning process, we would expect to observe a positive relationship between the recognition accuracy and IAT score differences between blocks:
When the recognition for briefly flashed words is high, IAT score differences should reflect the valence of the behavioral information but not with the word valence.
We would expect to observe smaller and eventually negative IAT score differences as the recognition accuracy declines and associative learning takes over.
To account for measurement error in the recognition accuracy of briefy flashed words we fit an errors-in-variable regression model [@Klauer_Draine_Greenwald_1998].
Because the model assumes that predictor values are sampled from a Gaussian distribution truncated at 0, we probit-transformed the recognition accuracy.^[A standard linear regression analysis yielded the same results.]

(ref:prime-recognition-iat-difference) Scatterplot of the recognition accuracy of briefly flashed words (on probit scale) and evaluative differences in IAT scores between learning blocks in which Bob was presented with positive descriptions and those in which he was paired with negative descriptions.
The regression line and confidence band represents predictions of the errors-in-variables model [@Klauer_Draine_Greenwald_1998].

```{r prime-recognition-iat-difference, fig.cap = "(ref:prime-recognition-iat-difference)", message = FALSE}
tmp <- lazyLoad(tools::file_path_sans_ext(list.files(pattern = "otm1-effects-plot_", recursive = TRUE)[1]))

otm1_iat_delta <- full_join(otm1_attitudes_delta, select(otm1_mem, ParticipantNumber, Accuracy)) %>% 
  filter(Measure == "IATscore")

otm1_iat_delta %>%
  ggplot(aes(x = Accuracy, y = delta)) +
  geom_hline(yintercept = 0, linetype = "22") +
  geom_vline(xintercept = 0.5, linetype = "22") +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "eiv_lm", color = "black") +
  labs(
    x = bquote("Recognition accuracy ["~Phi^-1*(italic(p))~"]")
    , y = expression("IAT score difference"~group("[", Delta~italic(z)~score, "]"))
  ) +
  scale_x_continuous(trans = "probit") +
  theme_apa()
```


```{r prime-recognition-iat-difference-regression, results = "asis"}
otm1_iat_delta_eiv_lm_res <- otm1_iat_delta %>%
  filter(Measure == "IATscore") %>%
  mutate(Accuracy = qnorm(Accuracy)) %>%
  eiv_lm(delta ~ Accuracy, data = ., n.optim = 10) %>% 
  apa_print
```


We did not detect a relationship between the recognition accuracy of briefly preseted words and IAT score differences between learning blocks, `r otm1_iat_delta_eiv_lm_res$full_result$Accuracy`.
Moreover, the positive intercept of the regression line indicates a positive IAT score difference despite at-chance word recognition accuracy, `r otm1_iat_delta_eiv_lm_res$full_result$Intercept`.
Hence, even for participants who exhibited no memory for briefly flashed words, IAT score differences reflected the valence of the behavioral information, see Figure\ \@ref(fig:prime-recognition-iat-difference).
These results provide no indication that the deviation of our findings from those reported by @rydell_two_2006 are attributable to the above-chance recognition of briefly flashed words in this study.


# Experiment 2

In the following we report the details of our a priori power analysis, provide details for the model specification used for the Bayesian model comparisons, and additional secondary analyses.

## Power analysis

As for Experiment 1, our power analyses focuses on this interaction and the theoretically relevant simple effects of *learning block*.
We estimate the sensitivity of our design using the R-package *Superpower* [@R-Superpower] for the contrast analyes using the R-package *emmeans* [@R-emmeans].
@rydell_two_2006 found the smallest learning block difference for IAT scores, $\hat\eta_p^2 = `r printp(rydell_pes)`$, $d_z \approx `r rydell_dz`$.
The learning block differences reported by @heycke_two_2018 were of similar magnitude but with an opposite sign.

```{r otm2-omnibus-power, cache = TRUE}
# Main effect block and three-way interaction
otm2_power <- function(f, n, rm_cor = 0.5, alpha = 0.05) {
  contrast_codes <- tibble::tibble(
    time = rep(contr.treatment(2), 4)
    , order = rep(rep(contr.sum(2), each = 2), 2)
    , block = rep(contr.sum(2), each = 4)
    , time_block = time * block
    , time_order = time * order
    , block_order = block * order
    , time_block_order = time * block * order
  )
  
  mu <- with(
    contrast_codes
    , -block_order * f +
      -block * 0.5 * f +
      time_block_order * f
  )
  
  string <- "2w*2b*2b"
  labelnames <- c("block", "1", "2", "order", "p-n", "n-p", "time", "13", "20")
  
  design_result <- ANOVA_design(
    design = string
    , n = n
    , mu = mu
    , sd = 1
    , r = rm_cor
    , labelnames = labelnames
  )
  
  ANOVA_exact(
    design_result
    , alpha_level = alpha_level
    , emm = FALSE
    , verbose = FALSE
  )
}


otm2_n <- 320/4

# 95% power
otm2_pes <- 0.03973071
otm2_dz <- pes_to_dz(otm2_pes, otm2_n, n_groups = 4)
rho <- 0.5
f <- dz_to_f(otm2_dz, rho)


# otm2_time_power <- otm2_power(f = f, n = otm2_n, rm_cor = rho, alpha = 0.05)
# otm2_time_power$main_result

otm2_dz_block <- f_to_dz(0.5*f, rho = rho)
otm2_pes_block <- dz_to_pes(otm2_dz_block, otm2_n, n_groups = 4)

otm2_dz_int <- f_to_dz(f, rho = rho)
otm2_pes_int <- dz_to_pes(otm2_dz_int, otm2_n, n_groups = 4)
```

```{r otm2-contrast-power, cache = TRUE}
otm2_power <- function(f, n, rm_cor = 0.5, alpha = 0.05) {
  contrast_codes <- tibble::tibble(
    time = rep(contr.treatment(2), 4)
    , order = rep(rep(contr.sum(2), each = 2), 2)
    , block = rep(contr.sum(2), each = 4)
    , time_block = time * block
    , time_order = time * order
    , block_order = block * order
    , time_block_order = time * block * order
  )
  
  mu <- with(
    contrast_codes
    , -order * 0.25*f +
      -block_order * f
  )
  
  string <- "2w*2b*2b"
  labelnames <- c("block", "1", "2", "order", "p-n", "n-p", "time", "13", "20")
  
  design_result <- ANOVA_design(
    design = string
    , n = n
    , mu = mu
    , sd = 1
    , r = rm_cor
    , labelnames = labelnames
  )
  
  ANOVA_exact(
    design_result
    , alpha_level = alpha_level
    , emm = TRUE
    , verbose = FALSE
    , contrast_type = "pairwise"
    , emm_comp = "block | time * order"
  )
}


# Learning block contrasts

## Simple contrasts
### 95% power
otm2_pes <- 0.03973071
otm2_dz <- pes_to_dz(otm2_pes, otm2_n, n_groups = 4)
# rho <- 0.5
# f <- dz_to_f(otm2_dz, rho)
# 
# otm2_contrast_power <- otm2_power(f = f, n = otm2_n, rm_cor = rho)$emmeans$contrasts %>%
#   emmeans_power(alpha = 0.05) %>%
#   .[1, ]
# otm2_contrast_power


### 80% power
otm2_pes2 <- 0.0243813
otm2_dz2 <- pes_to_dz(otm2_pes2, otm2_n, n_groups = 4)
# f <- dz_to_f(otm2_dz2, rho)
# 
# otm2_contrast_power2 <- otm2_power(f = f, n = otm2_n, rm_cor = rho)$emmeans$contrasts %>%
#   emmeans_power(alpha = 0.05) %>%
#   .[1, ]
# otm2_contrast_power2


## Collapsed once
### 95% power
otm2_dz_between_collapsed <- f_to_dz(dz_to_f(otm2_dz, rho) / sqrt(2), rho)
otm2_pes_between_collapsed <- dz_to_pes(otm2_dz_between_collapsed, otm2_n, n_groups = 4)
# f <- dz_to_f(otm2_dz_between_collapsed, rho)
# 
# otm2_time_collapsed_contrast_power <- otm2_power(f = f, n = otm2_n, rm_cor = rho)$emmeans$emmeans %>%
#   emmeans(specs = ~ block | order) %>%
#   pairs %>%
#   emmeans_power(alpha = 0.05) %>%
#   .[1, ]
# otm2_time_collapsed_contrast_power

### 80% power
otm2_dz2_between_collapsed <- f_to_dz(dz_to_f(otm2_dz2, rho) / sqrt(2), rho)
otm2_pes2_between_collapsed <- dz_to_pes(otm2_dz2_between_collapsed, otm2_n, n_groups = 4)
# f <- dz_to_f(otm2_dz2_between_collapsed, rho)
# 
# otm2_time_collapsed_contrast_power2 <- otm2_power(f = f, n = otm2_n, rm_cor = rho)$emmeans$emmeans %>%
#   emmeans(specs = ~ block | order) %>%
#   pairs %>%
#   emmeans_power(alpha = 0.05) %>%
#   .[1, ]
# otm2_time_collapsed_contrast_power2


## Collapsed twice
### 95% power
otm2_dz_collapsed <- f_to_dz(dz_to_f(otm2_dz, rho) / sqrt(4), rho)
otm2_pes_collapsed <- dz_to_pes(otm2_dz_collapsed, otm2_n, n_groups = 4)
# f <- dz_to_f(otm2_dz_collapsed, rho)
# 
# otm2_collapsed_contrast_power <- otm2_power(f = f, n = otm2_n, rm_cor = rho)$emmeans$emmeans %>%
#   emmeans(specs = ~ block * order) %>%
#   contrast(method = list(collapsed = c(-0.5, 0.5, 0.5, -0.5))) %>%
#   emmeans_power(alpha_level = 0.05)
# otm2_collapsed_contrast_power

### 80% power
otm2_dz2_collapsed <- f_to_dz(dz_to_f(otm2_dz2, rho) / sqrt(4), rho)
otm2_pes2_collapsed <- dz_to_pes(otm2_dz2_collapsed, otm2_n, n_groups = 4)
# f <- dz_to_f(otm2_dz2_collapsed, rho)
# 
# otm2_collapsed_contrast_power2 <- otm2_power(f = f, n = otm2_n, rm_cor = rho)$emmeans$emmeans %>%
#   emmeans(specs = ~ block * order) %>%
#   contrast(method = list(collapsed = c(-0.5, 0.5, 0.5, -0.5))) %>%
#   emmeans_power(alpha_level = 0.05)
# otm2_collapsed_contrast_power2
```

Across all locations we will recruit N = 320 participants (not including participants from the pilot study).
To maximize the power of the planned contrasts, we will test whether valence order moderates the learning block contrasts by testing the main effect of learning block ($\alpha = \beta = .05$ for $\delta_z = `r otm2_dz_block`$ and $N = 320$).
If we detect no main effect of learning block, we will pool participants across valence orders by reversing the learning block coding in one group (as in the Bayesian model comparison of Experiment 1).
Similarly, if the different prime presentation durations do not moderate the learning block contrasts, we will pool participants across presentation durations ($\alpha = \beta = .05$ for $\delta_z = `r otm2_dz_int`$ and $N = 320$).
This analysis plan results in four possible contrast analyses of the learning blocks: (1) 4 learning block contrasts by valence order and presentation duration, (2) 2 learning block contrasts collapsed across valence orders but separate for presentation durations, (3) 2 learning block contrasts collapsed across presentation durations but separate for valence orders, and (4) 1 learning block contrasts collapsed across valence orders and presentation durations.

The data from all locations (N = 320, not including participants from the pilot study) provides 95% power to detect learning block differences as small as $\eta^2_p = `r printp(otm2_pes)`$ ($\delta_z = `r otm2_dz`$) or as small as $\eta^2_p = `r printp(otm2_pes_between_collapsed)`$ ($\delta_z = `r otm2_dz_between_collapsed`$) and $\eta^2_p = `r printp(otm2_pes_collapsed)`$ ($\delta_z = `r otm2_dz_collapsed`$) when pooling participants across one or both between-participant factors ($\alpha = .05$).
Thus, our design is sufficiently sensitive to detect (or rule out) differences `r round((1 - otm2_dz/rydell_dz) * 100)`% smaller (`r round((1 - otm2_dz_between_collapsed/rydell_dz) * 100)`% or `r round((1 - otm2_dz_collapsed/rydell_dz) * 100)`% when pooling participants across one or both between-participant factors, respectively) than the smallest learning block difference reported by @rydell_two_2006.
Figure\ \@ref(fig:otm2-power-curves) illustrates the implied sensitivity in units of Cohen's $\delta$ depending on the assumed repeated-measures correlation $\rho$.


(ref:otm2-power-curves) Sensitivity curves for learning block contrasts in Experiment 2 depending on the assumed repeated measures correlation.
Simple contrast are estimated within each *flashed word presentation duration*-*valence order* combination (not adjusted for multiple comparisons).
Collapsed contrasts pool participants across one or both  of the between subject factors (i.e., *flashed word presentation duration* and *valence order*).
$\delta_z$-values in the legend correspond to 95% and 80% power.
The grey dashed line at the top represents the smallest estimate of the learning block difference reported by Rydell et al. [$d_z = 0.47$; -@rydell_two_2006].

```{r otm2-power-curves, fig.width = 7, fig.height = 4, fig.cap = "(ref:otm2-power-curves)"}
rydell_power_curve <- tibble(
  rho = seq(-1, 1, length.out = 100)
  , delta = c(
    dz_to_f(rydell_dz, unique(rho)) * 2
  )
  , power = NA
)

otm1_power_curve <- tibble(
  rho = rep(seq(-1, 1, length.out = 100), 2)
  , delta = c(
    dz_to_f(otm1_dz, unique(rho)) * 2
    , dz_to_f(otm1_dz2, unique(rho)) * 2
  )
  , power = rep(c("0.95", "0.80"), each = length(rho)/2)
)

tibble(
  rho = rep(seq(-1, 1, length.out = 100), 2)
  , otm2_delta_simple = c(
    dz_to_f(otm2_dz, unique(rho)) * 2
    , dz_to_f(otm2_dz2, unique(rho)) * 2
  )
  , otm2_delta_between_collapsed = c(
    dz_to_f(otm2_dz_between_collapsed, unique(rho)) * 2
    , dz_to_f(otm2_dz2_between_collapsed, unique(rho)) * 2
  )
  , otm2_delta_collapsed = c(
    dz_to_f(otm2_dz_collapsed, unique(rho)) * 2
    , dz_to_f(otm2_dz2_collapsed, unique(rho)) * 2
  )
  , power = rep(c("0.95", "0.80"), each = length(rho)/2)
) %>% 
  pivot_longer(cols = matches("otm"), names_to = "contrast", values_to = "delta") %>%
  mutate(
    power = relevel(factor(power), "0.95")
    , contrast = gsub("otm2_delta_", "", contrast)
    , contrast = gsub("_", " ", contrast)
    , contrast = factor(contrast, levels = c("simple", "between collapsed", "collapsed"))
  ) %>% 
  ggplot(aes(x = rho, y = delta, color = contrast, group = interaction(power, contrast), linetype = power)) +
  geom_line(data = rydell_power_curve, aes(group = power), color = grey(0.8), linetype = "dashed") +
  geom_line() +
  scale_color_viridis_d(
    end = 0.8
    , labels = c(
      bquote("Simple                 "~delta[z] %in% "{"*.(printnum(otm2_dz))*", "*.(printnum(otm2_dz2))*"}")
      , bquote("Collapsed once   "~delta[z] %in% "{"*.(printnum(otm2_dz_between_collapsed))*", "*.(printnum(otm2_dz2_between_collapsed))*"}")
      , bquote("Collapsed twice   "~delta[z] %in% "{"*.(printnum(otm2_dz_collapsed))*", "*.(printnum(otm2_dz2_collapsed))*"}")
    )
  ) +
  ylim(c(0, 1)) +
  labs(
    x = bquote("Correlation between learning blocks ["*rho*"]")
    , y = bquote("Learning block difference [Cohen's"~delta*"]")
    , color = "Contrast"
    , linetype = bquote("Power"~"["*1 - beta*"]")
  ) +
  ggtitle("", subtitle = bquote(list(alpha ==~".05", italic(N) == .(otm2_n*4)))) +
  theme_apa(box = TRUE) + 
  theme(
    panel.grid.major.x = element_line()
    , panel.grid.major.y = element_line()
    , panel.grid.minor.x = element_line()
    , panel.grid.minor.y = element_line()
    , legend.justification = "top"
  )
```

```{r create_r-references}
r_refs(file = "r-references.bib")
```

\newpage

# References

\begingroup
\setlength{\parskip}{0pt}
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
